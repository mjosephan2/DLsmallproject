{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "dir = \"dataset/train\"\n",
    "train_normal=len(os.listdir(os.path.join(dir,\"normal\")))\n",
    "train_infected_covid=len(os.listdir(os.path.join(dir,\"infected\",\"covid\")))\n",
    "train_infected_non_covid=len(os.listdir(os.path.join(dir,\"infected\",\"non-covid\")))\n",
    "\n",
    "# val\n",
    "dir = \"dataset/val\"\n",
    "val_normal=len(os.listdir(os.path.join(dir,\"normal\")))\n",
    "val_infected_covid=len(os.listdir(os.path.join(dir,\"infected\",\"covid\")))\n",
    "val_infected_non_covid=len(os.listdir(os.path.join(dir,\"infected\",\"non-covid\")))\n",
    "\n",
    "# test\n",
    "dir = \"dataset/test\"\n",
    "test_normal=len(os.listdir(os.path.join(dir,\"normal\")))\n",
    "test_infected_covid=len(os.listdir(os.path.join(dir,\"infected\",\"covid\")))\n",
    "test_infected_non_covid=len(os.listdir(os.path.join(dir,\"infected\",\"non-covid\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Normal: 1341\n",
      "Infected: 3875\n",
      "Infected covid: 1345\n",
      "Infected non covid: 2530\n",
      "============================================================\n",
      "Validation\n",
      "Normal: 8\n",
      "Infected: 17\n",
      "Infected covid: 9\n",
      "Infected non covid: 8\n",
      "============================================================\n",
      "Test\n",
      "Normal: 234\n",
      "Infected: 381\n",
      "Infected covid: 139\n",
      "Infected non covid: 242\n"
     ]
    }
   ],
   "source": [
    "# print\n",
    "print(\"Train\")\n",
    "print(\"Normal:\", train_normal)\n",
    "print(\"Infected:\", train_infected_covid + train_infected_non_covid)\n",
    "print(\"Infected covid:\", train_infected_covid)\n",
    "print(\"Infected non covid:\", train_infected_non_covid)\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Validation\")\n",
    "print(\"Normal:\", val_normal)\n",
    "print(\"Infected:\", val_infected_covid + val_infected_non_covid)\n",
    "print(\"Infected covid:\", val_infected_covid)\n",
    "print(\"Infected non covid:\", val_infected_non_covid)\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Test\")\n",
    "print(\"Normal:\", test_normal)\n",
    "print(\"Infected:\", test_infected_covid + test_infected_non_covid)\n",
    "print(\"Infected covid:\", test_infected_covid)\n",
    "print(\"Infected non covid:\", test_infected_non_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images in each part of the dataset\n",
    "dataset_numbers = {'train_normal': 1341,\\\n",
    "                   'train_infected': 3875,\\\n",
    "                   'train_infected_covid': 1345,\\\n",
    "                   'train_infected_non_covid': 2530, \\\n",
    "                   'val_normal': 8,\\\n",
    "                   'val_infected': 17, \\\n",
    "                   'val_infected_covid': 9,\\\n",
    "                   'val_infected_non_covid': 8,\\\n",
    "                   'test_normal': 234, \\\n",
    "                   'test_infected': 381, \\\n",
    "                   'test_infected_covid': 139, \\\n",
    "                   'test_infected_non_covid': 242}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model - AlexNet\n",
    "We will primarily use AlexNet for both binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 1, 1), #in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.MaxPool2d(3), #kernel_size\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(96, 256, 5, padding = 0),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 384, 3, padding = 0),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(384, 384, 3, padding = 0),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(384, 256, 3, padding = 0),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*2*2 , 4096),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "OUTPUT_DIM = 2\n",
    "model = AlexNet(OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix a seed\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 24,711,874 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): ReLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels, classes, normalize = False):\n",
    "\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        \n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image_min = image.min()\n",
    "            image_max = image.max()\n",
    "            image.clamp_(min = image_min, max = image_max)\n",
    "            image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(classes[labels[i]])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for two binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to pick the normal & infected (combined) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f7867bb4080>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f7867bb4668>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f7867bb42e8>\n"
     ]
    }
   ],
   "source": [
    "from dataloader import Binary_Lung_Dataset\n",
    "\n",
    "# Test\n",
    "bs = 16\n",
    "ld_train = Binary_Lung_Dataset(groups=\"train\",  classify=\"normal\")\n",
    "ld_val = Binary_Lung_Dataset(groups=\"val\", classify=\"normal\")\n",
    "ld_test = Binary_Lung_Dataset(groups=\"test\",  classify=\"normal\")\n",
    "train_loader = DataLoader(ld_train, batch_size = bs, shuffle = True)\n",
    "test_loader = DataLoader(ld_test, batch_size = bs, shuffle = True)\n",
    "val_loader = DataLoader(ld_val, batch_size = bs, shuffle = True)\n",
    "\n",
    "print(train_loader)\n",
    "print(val_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n",
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n",
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "for k, v in enumerate(train_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break\n",
    "\n",
    "for k, v in enumerate(test_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break\n",
    "\n",
    "for k, v in enumerate(val_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, torch.max(y,1)[1])\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, torch.max(y,1)[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred,y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_LR = 1e-7\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -1995.647 | Train Acc: 74.29%\n",
      "\t Val. Loss: -1723.940 |  Val. Acc: 61.88%\n",
      "Epoch: 02 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -2325.312 | Train Acc: 74.29%\n",
      "\t Val. Loss: -2022.510 |  Val. Acc: 62.09%\n",
      "Epoch: 03 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -2712.055 | Train Acc: 74.29%\n",
      "\t Val. Loss: -2368.600 |  Val. Acc: 61.88%\n",
      "Epoch: 04 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -3173.330 | Train Acc: 74.29%\n",
      "\t Val. Loss: -2784.061 |  Val. Acc: 62.29%\n",
      "Epoch: 05 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -3706.755 | Train Acc: 74.29%\n",
      "\t Val. Loss: -3262.450 |  Val. Acc: 62.29%\n",
      "Epoch: 06 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -4332.899 | Train Acc: 74.29%\n",
      "\t Val. Loss: -3813.598 |  Val. Acc: 61.88%\n",
      "Epoch: 07 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -5055.871 | Train Acc: 74.29%\n",
      "\t Val. Loss: -4470.702 |  Val. Acc: 62.09%\n",
      "Epoch: 08 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -5885.456 | Train Acc: 74.29%\n",
      "\t Val. Loss: -5198.708 |  Val. Acc: 61.68%\n",
      "Epoch: 09 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -6833.091 | Train Acc: 74.29%\n",
      "\t Val. Loss: -6080.583 |  Val. Acc: 62.09%\n",
      "Epoch: 10 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -7926.939 | Train Acc: 74.29%\n",
      "\t Val. Loss: -7055.952 |  Val. Acc: 61.88%\n",
      "Epoch: 11 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -9171.271 | Train Acc: 74.29%\n",
      "\t Val. Loss: -8160.199 |  Val. Acc: 61.47%\n",
      "Epoch: 12 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -10585.546 | Train Acc: 74.29%\n",
      "\t Val. Loss: -9428.631 |  Val. Acc: 61.68%\n",
      "Epoch: 13 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -12190.948 | Train Acc: 74.29%\n",
      "\t Val. Loss: -10911.022 |  Val. Acc: 61.88%\n",
      "Epoch: 14 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -13957.731 | Train Acc: 74.29%\n",
      "\t Val. Loss: -12525.930 |  Val. Acc: 61.68%\n",
      "Epoch: 15 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -15979.300 | Train Acc: 74.29%\n",
      "\t Val. Loss: -14462.670 |  Val. Acc: 62.50%\n",
      "Epoch: 16 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -18266.879 | Train Acc: 74.29%\n",
      "\t Val. Loss: -16524.206 |  Val. Acc: 62.29%\n",
      "Epoch: 17 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -20864.442 | Train Acc: 74.29%\n",
      "\t Val. Loss: -18877.795 |  Val. Acc: 62.29%\n",
      "Epoch: 18 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -23669.393 | Train Acc: 74.29%\n",
      "\t Val. Loss: -21491.134 |  Val. Acc: 62.09%\n",
      "Epoch: 19 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -26863.319 | Train Acc: 74.29%\n",
      "\t Val. Loss: -24348.197 |  Val. Acc: 61.68%\n",
      "Epoch: 20 - Epoch Duration: 0m 12s\n",
      "\tTrain Loss: -30467.391 | Train Acc: 74.29%\n",
      "\t Val. Loss: -27778.303 |  Val. Acc: 62.29%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'two_binary_classifier_1_alexnet.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} - Epoch Duration: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, torch.max(y,1)[1])\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, torch.max(y,1)[1])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = 'two_binary_classifier_1_alexnet.pt'\n",
    "output_dim = 2\n",
    "\n",
    "# Load from saved model\n",
    "model = AlexNet(output_dim)\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-27526.669759114582, 0.6356169879436493)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Load from saved model and evaluate\n",
    "evaluation_results = evaluate(model, test_loader, criterion, device)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to pick the infected data set consisting of covid & non-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f78676b37b8>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f7867bb44e0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f78676b3550>\n"
     ]
    }
   ],
   "source": [
    "from dataloader import Binary_Lung_Dataset\n",
    "\n",
    "# Test\n",
    "bs = 16\n",
    "ld_train = Binary_Lung_Dataset(groups=\"train\",  classify=\"infected\")\n",
    "ld_val = Binary_Lung_Dataset(groups=\"val\", classify=\"infected\")\n",
    "ld_test = Binary_Lung_Dataset(groups=\"test\",  classify=\"infected\")\n",
    "train_loader = DataLoader(ld_train, batch_size = bs, shuffle = True)\n",
    "test_loader = DataLoader(ld_test, batch_size = bs, shuffle = True)\n",
    "val_loader = DataLoader(ld_val, batch_size = bs, shuffle = True)\n",
    "\n",
    "print(train_loader)\n",
    "print(val_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n",
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n",
      "-----\n",
      "0\n",
      "torch.Size([16, 1, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "for k, v in enumerate(train_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break\n",
    "\n",
    "for k, v in enumerate(test_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break\n",
    "\n",
    "for k, v in enumerate(val_loader):\n",
    "    print(\"-----\")\n",
    "    print(k)\n",
    "    print(v[0].shape)\n",
    "    # Forced stop\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions are written above in Binary Classfier #1 already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_LR = 1e-7\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - Epoch Time: 0m 10s\n",
      "\tTrain Loss: -141920.464 | Train Acc: 65.18%\n",
      "\t Val. Loss: -138267.469 |  Val. Acc: 63.62%\n",
      "Epoch: 02 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -152703.510 | Train Acc: 65.29%\n",
      "\t Val. Loss: -148712.132 |  Val. Acc: 63.62%\n",
      "Epoch: 03 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -164244.611 | Train Acc: 65.29%\n",
      "\t Val. Loss: -159828.820 |  Val. Acc: 63.56%\n",
      "Epoch: 04 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -176023.972 | Train Acc: 65.29%\n",
      "\t Val. Loss: -171474.725 |  Val. Acc: 63.38%\n",
      "Epoch: 05 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -189103.850 | Train Acc: 65.29%\n",
      "\t Val. Loss: -184232.952 |  Val. Acc: 63.50%\n",
      "Epoch: 06 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -202725.753 | Train Acc: 65.29%\n",
      "\t Val. Loss: -197547.882 |  Val. Acc: 63.38%\n",
      "Epoch: 07 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -217177.383 | Train Acc: 65.18%\n",
      "\t Val. Loss: -211912.485 |  Val. Acc: 63.56%\n",
      "Epoch: 08 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -232423.012 | Train Acc: 65.41%\n",
      "\t Val. Loss: -226580.790 |  Val. Acc: 63.32%\n",
      "Epoch: 09 - Epoch Time: 0m 8s\n",
      "\tTrain Loss: -248633.719 | Train Acc: 65.29%\n",
      "\t Val. Loss: -242924.749 |  Val. Acc: 63.56%\n",
      "Epoch: 10 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -265519.327 | Train Acc: 65.29%\n",
      "\t Val. Loss: -259842.277 |  Val. Acc: 63.50%\n",
      "Epoch: 11 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -284174.470 | Train Acc: 65.41%\n",
      "\t Val. Loss: -277390.009 |  Val. Acc: 63.32%\n",
      "Epoch: 12 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -303750.170 | Train Acc: 65.41%\n",
      "\t Val. Loss: -296810.014 |  Val. Acc: 63.50%\n",
      "Epoch: 13 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -324025.056 | Train Acc: 65.29%\n",
      "\t Val. Loss: -317269.107 |  Val. Acc: 63.74%\n",
      "Epoch: 14 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -345363.869 | Train Acc: 65.29%\n",
      "\t Val. Loss: -338171.018 |  Val. Acc: 63.62%\n",
      "Epoch: 15 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -367791.296 | Train Acc: 65.18%\n",
      "\t Val. Loss: -360568.182 |  Val. Acc: 63.50%\n",
      "Epoch: 16 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -392648.329 | Train Acc: 65.29%\n",
      "\t Val. Loss: -384236.560 |  Val. Acc: 63.50%\n",
      "Epoch: 17 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -417549.632 | Train Acc: 65.18%\n",
      "\t Val. Loss: -408934.637 |  Val. Acc: 63.38%\n",
      "Epoch: 18 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -443691.602 | Train Acc: 65.18%\n",
      "\t Val. Loss: -435599.253 |  Val. Acc: 63.50%\n",
      "Epoch: 19 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -472265.091 | Train Acc: 65.41%\n",
      "\t Val. Loss: -463122.418 |  Val. Acc: 63.44%\n",
      "Epoch: 20 - Epoch Time: 0m 9s\n",
      "\tTrain Loss: -501510.145 | Train Acc: 65.29%\n",
      "\t Val. Loss: -493190.460 |  Val. Acc: 63.68%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'two_binary_classifier_2_alexnet.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} - Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluation\n",
    "output_dim = 2\n",
    "saved_model_path = 'two_binary_classifier_2_alexnet.pt'\n",
    "\n",
    "\n",
    "# Load from saved model\n",
    "model = AlexNet(output_dim)\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-27567.50244140625, 0.6374198744694392)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Load from saved model and evaluate\n",
    "evaluation_results = evaluate(model, test_loader, criterion, device)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "output_dim = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Model 1\n",
    "saved_model_path_1 = 'two_binary_classifier_1_alexnet.pt'\n",
    "# Load from saved model\n",
    "model1 = AlexNet(output_dim)\n",
    "model1.load_state_dict(torch.load(saved_model_path_1))\n",
    "model1 = model1.to(device)\n",
    "\n",
    "\n",
    "# Model 2\n",
    "saved_model_path_2 = 'two_binary_classifier_2_alexnet.pt'\n",
    "# Load from saved model\n",
    "model2 = AlexNet(output_dim)\n",
    "model2.load_state_dict(torch.load(saved_model_path_2))\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
